Title: Un robot au labo
Author: Edwin Cartlidge
Abstract: Des machines automatisent des expériences scientifiques répétitives. D’autres vont plus loin: elles élaborent elles-mêmes des hypothèses inédites et les testent par la suite.

Un bras robotique tire le portrait d’un visiteur. Le stylo ne tremble pas, mais l’algorithme qu’il suit à la lettre ne laisse aucune place à l’interprétation. | Photo: robotlab (2002)
Effectuer une expérience scientifique sans quitter son bureau, remplir la moindre éprouvette ou jeter un œil dans un microscope. C’est la vision d’Emerald Cloud Laboratory: la société offre aux chercheurs de mener leurs travaux via Internet. Biologistes et chimistes peuvent formuler les protocoles et les superviser, modifier les paramètres des réactions et analyser les résultats. De quoi étendre le concept de cloud computing de la simple gestion des données au contrôle de processus à distance. L’idée est de libérer les chercheurs des travaux astreignants afin de leur laisser le temps de concevoir de meilleures expériences.
Ce service est fourni depuis un entrepôt de la région de San Francisco. Alignés en rang sur des bancs d’essai, des robots manipulateurs de liquides, des incubateurs automatisés et des centrifugeuses traitent les échantillons selon les instructions détaillées soumises par les utilisateurs via une interface en ligne. Les automates travaillent en continu de manière plus ou moins autonome, et les résultats arrivent sur l’ordinateur du chercheur en général dans les 24 heures suivant la requête.
Il n’existe que peu d’entreprises à offrir ce type de prestations. La première, Transcriptic, a été fondée en 2012 et a ses locaux à une vingtaine de kilomètres d’Emerald. La communauté scientifique compte déjà des enthousiastes. Justin Siegel, expert en biologie synthétique à l’Université de Californie à Davis, dit que ses étudiants chercheurs sont ainsi en mesure de tester davantage d’hypothèses et en formuler de plus audacieuses que s’ils effectuaient eux-mêmes les expériences. Cela permet aussi à des étudiants des degrés inférieurs de participer. «Ils peuvent se concentrer sur l’élaboration des expériences sans s’inquiéter de leur habileté à les exécuter», explique-t-il.
Emerald pourrait être victime de son propre succès. Son service cloud a démarré en octobre 2016, et plusieurs centaines de laboratoires sont sur une liste d’attente. Son co-fondateur, Brian Frezza se dit confiant: il estime que société sera en mesure d’effectuer d’ici un an environ les cent expériences les plus courantes dans les sciences de la vie. Elle en offre actuellement une quarantaine. «Nous voulons être rentables à ce moment-là», dit-il.
La recherche connaît déjà bien les robots. Depuis des années, les laboratoires pharmaceutiques les utilisent pour des tâches longues et répétitives au début du développement d’un médicament. La biotechnologie y recourt pour manipuler l’ADN – un domaine en forte expansion où des fabricants d’appareils tels que Tecan, basé près de Zurich, satisfont la demande. «Les machines peuvent maintenant exécuter presque toutes les tâches qu’un humain effectue en laboratoire», explique Ross King, biologiste et informaticien à l’Université de Manchester en Grande-Bretagne.
L’avantage: la reproductibilité
Le séquençage de l’ADN constitue probablement l’archétype de l’automatisation dans les sciences. Par le passé, seuls quelques laboratoires assuraient le travail fastidieux nécessaire pour déterminer l’ordre des paires de bases nucléiques, aujourd’hui effectué par des automates à même de lire le matériel génétique des millions de fois. Ils ont été centralisés dans des plateformes technologiques, et les laboratoires assurent rarement eux-mêmes cette procédure, note Justin Siegel.
Ce que réalise Emerald est «fondamentalement différent», argue Brian Frezza. Au lieu de répéter «peut-être un million de fois la même expérience comme dans une usine automobile, nous faisons un million d’expériences différentes à la fois». Mais l’entreprise n’a pas l’ambition de concurrencer les prix des sociétés de recherche contractuelle qui combinent travail humain et automatisé. Car ses robots ont de la peine avec les séries d’étapes successives (au contraire de procédures analogues menées simultanément). Au final, ils travaillent en général moins vite que des laborantins et reviennent plus cher.
Le grand avantage offert par les robots est la reproductibilité, avance Brian Frezza, car ils «manipulent toujours leurs pipettes exactement de la même manière». Il a fallu pour cela développer un jeu d’instructions qui permette aux scientifiques de définir exactement et sans la moindre ambiguïté les différentes étapes à accomplir par le robot pour une expérience donnée. Brian Frezza dit avoir réussi, après des années de travail, à développer un ensemble robuste de commandes, mais l’interface doit devenir plus conviviale. «L’idée d’écrire en code tend à rebuter les gens.»
Richard Whitby, chercheur à l’Université de South ampton en Grande-Bretagne, souligne lui aussi l’importance de la reproductibilité. La flexibilité des humains représente certes un avantage de taille pour réaliser des réactions complexes dans son domaine, la chimie organique. Pourtant, les publications scientifiques rendent souvent mal compte de cette complexité: ils omettent par exemple de préciser à quelle vitesse un réactif a été ajouté. Sans connaître tous les paramètres d’une réaction, il est difficile de quantifier l’effet de la modification d’une variable précise dans le but d’améliorer le processus chimique, note le chercheur.
Le chimiste dirige le projet Dial-a-Molecule, qui veut construire une machine capable de synthétiser à la demande n’importe quel composé organique – à l’instar de ce qui se passe aujourd’hui en génétique, où des fragments spécifiques d’ADN peuvent être simplement commandés par la poste. Richard Whitby ne se fait pas d’illusions sur la difficulté du projet: l’appareil devra réaliser des dizaines de milliers de réactions différentes, contre seulement quatre dans le cas des synthétiseurs d’ADN. Il se donne «de trente à quarante ans» pour y parvenir.
Les hypothèses automatiques
La vision de Ross King à Manchester est encore plus ambitieuse: il veut «automatiser le cycle complet de la recherche scientifique». Son équipe utilise, comme les entreprises du cloud, des robots commerciaux mais va plus loin en les connectant à des systèmes d’intelligence artificielle. Formé dans un domaine spécifique par la logique et la théorie des probabilités, un robot devra de lui-même élaborer des hypothèses afin d’expliquer une série d’observations. Dans un deuxième temps, le système développe des expériences afin de tester ses hypothèses, qu’il mène tout seul, avant de générer de nouvelles pistes. Le cycle pourra se répéter plusieurs fois dans le but d’apprendre quelque chose de nouveau sur le monde.
Ces idées, Ross King les a développées à l’Université d’Aberystwyth, dans le pays de Galles. Il y a construit le robot Adam qui a identifié en 2008 de nouveaux gènes responsables de l’encodage de certaines enzymes de la levure. Le chercheur s’est ensuite attelé à Eve, un robot d’un million de dollars. Ce dernier a découvert le mécanisme antipaludéen du triclosan, un produit antibactérien et antifongique courant, et ainsi ouvert la voie vers son éventuelle homologation.
La science des matériaux recourt elle aussi de plus en plus aux robots. En 2016, les ingénieurs du laboratoire de recherche de l’armée de l’air américaine dans l’Ohio ont présenté les résultats d’essais effectués par un robot doté d’intelligence artificielle sur des nanotubes de carbone, des molécules cylindriques résistantes, légères et bonnes conductrices de chaleur et d’électricité. La machine a réalisé de manière autonome plus de 600 expériences où elle a modifié les paramètres pour essayer d’accélérer la croissance des nanotubes. Ses résultats ont confirmé des prévisions théoriques sur le taux de croissance maximal.
D’autres scientifiques veulent même automatiser les découvertes en physique, bien qu’ils n’utilisent pas des robots en tant que tels. Hod Lipson de l’Université Cornell a développé un algorithme qui génère au hasard des équations avant de sélectionner, selon un processus évolutionniste, celles qui correspondent le mieux à des données expérimentales. En 2009, son équipe a annoncé avoir formulé de cette manière un modèle du pendule double (un système mécanique relativement simple) et être parvenue à ce qu’elle décrit comme des lois de conservation pertinentes en physique. Deux ans plus tard, elle a utilisé des données sur le métabolisme de la levure pour dériver des équations décrivant l’énergie libérée par la dégradation du sucre.
Mais tout le monde n’est pas convaincu. Dans une lettre envoyée en 2009 à la revue Science, les physiciens américains Philip Anderson et Elihu Abrahams ont accusé les équipes de Ross King et de Hop Lipson de «se tromper gravement sur la nature de l’entreprise scientifique». Ils relèvent que les machines contribuent à ce que le philosophe Tomas Kuhn appelle la science normale, mais ne découvriront jamais de nouvelles lois physiques capables de la transformer. Ils soutiennent que «les lois physiques pertinentes et les variables sont connues d’avance» dans les recherches de Hop Lipson sur les pendules.
Ross King reconnaît que les machines ont leurs limites et relève qu’un robot qui mène à bien une expérience ne sait pas pourquoi il le fait. Il raconte aussi que son équipe souhaitait inclure Adam et Eve dans la liste des auteurs de leurs contributions mais qu’on leur a répondu que c’était impossible parce qu’ils ne pouvaient pas donner leur accord. Il ne doute cependant pas que les automates intelligents prendront toujours plus de place dans les sciences, portés par le pouvoir croissant des ordinateurs, de meilleurs algorithmes et les progrès de la robotique. «Les machines s’améliorent sans cesse alors que les hommes ne changent pas, dit-il. Il n’y a pas de raison que cette évolution s’arrête.»
Basé à Rome, Edwin Cartlidge écrit pour les revues Science et Nature.